# Benchmark System

Comprehensive benchmark for comparing action model learning algorithms:
- **PI-SAM** (Standard SAM learning)
- **Noisy PI-SAM** (Conflict-driven patch search)
- **ROSAME** (Robust Statistical Action Model Estimation)

## Quick Start

### 1. Generate Training Data

```bash
python benchmark/data_generator.py --domain blocks --num-steps 100 --trace-length 15
```

This creates:
- 100-step trajectory for ROSAME (101 images)
- 6 non-overlapping 15-step traces for our algorithms

### 2. Add LLM Noise

```bash
python benchmark/noise_generator.py --domain blocks
```

This processes all images with LLM vision and creates:
- `.trajectory` and `.masking_info` files for all traces
- Probability observations for ROSAME

**Note**: This step takes 10-20+ minutes for 101 LLM API calls.

### 3. Run Experiments

```bash
python benchmark/experiment_runner.py --domain blocks
```

This runs all three algorithms on 14 test problems and collects metrics.

### 4. Generate Results

```bash
python benchmark/results_analyzer.py --domain blocks
```

This creates CSV files and comparison plots.

## Directory Structure

```
benchmark/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ data_generator.py            # Generate training trajectories
â”œâ”€â”€ noise_generator.py           # Add LLM vision noise
â”œâ”€â”€ experiment_runner.py         # Run all experiments
â”œâ”€â”€ results_analyzer.py          # Analyze and visualize results
â”‚
â”œâ”€â”€ domains/
â”‚   â””â”€â”€ blocks/
â”‚       â”œâ”€â”€ blocks_no_handfull.pddl          # Equalized domain
â”‚       â”œâ”€â”€ prompts.py                        # LLM prompts (no handfull)
â”‚       â””â”€â”€ equalized_fluent_classifier.py    # Fluent classifier
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ blocks/
â”‚       â””â”€â”€ training/
â”‚           â”œâ”€â”€ rosame_trace/                 # 100-step trace
â”‚           â”‚   â”œâ”€â”€ images/                   # 101 images
â”‚           â”‚   â”œâ”€â”€ problem1.trajectory       # Full LLM trajectory
â”‚           â”‚   â”œâ”€â”€ problem1.masking_info     # Full masking
â”‚           â”‚   â””â”€â”€ rosame_probability_observations.json
â”‚           â””â”€â”€ our_algorithms_traces/
â”‚               â”œâ”€â”€ trace_0/                  # States 0-15
â”‚               â”‚   â”œâ”€â”€ images/               # 16 images
â”‚               â”‚   â”œâ”€â”€ trace_metadata.json
â”‚               â”‚   â”œâ”€â”€ problem1.trajectory   # Split from ROSAME
â”‚               â”‚   â””â”€â”€ problem1.masking_info # Split from ROSAME
â”‚               â”œâ”€â”€ trace_1/                  # States 15-30
â”‚               â””â”€â”€ ...
â”‚
â””â”€â”€ results/
    â””â”€â”€ blocks/
        â”œâ”€â”€ benchmark_results.json
        â”œâ”€â”€ pisam/
        â”‚   â”œâ”€â”€ learned_model.pddl
        â”‚   â””â”€â”€ results.json
        â”œâ”€â”€ noisy_pisam/
        â”‚   â”œâ”€â”€ learned_model.pddl
        â”‚   â””â”€â”€ results.json
        â””â”€â”€ rosame/
            â”œâ”€â”€ learned_model.pddl
            â””â”€â”€ results.json
```

## Test Problems

The benchmark uses 14 test problems:

**From `pddl/blocks` (4 problems)**:
- problem3.pddl, problem5.pddl, problem7.pddl, problem9.pddl
- (Excludes problem1.pddl which was used for training)

**From `pddl/blocks_test` (5 problems)**:
- problem2.pddl, problem4.pddl, problem6.pddl, problem8.pddl, problem10.pddl

**From `pddl/blocks_medium` (5 problems)**:
- problem0.pddl, problem2.pddl, problem3.pddl, problem4.pddl, problem5.pddl

## Key Features

### Domain Equalization
All experiments use the equalized blocks domain **without the `handfull` predicate**, matching ROSAME's definition.

### Consistent LLM Classifications
All training traces use the **same LLM classifications** from the ROSAME processing, ensuring fair comparison.

### Trajectory Splitting
Our algorithm traces are **split from the full ROSAME trajectory**:
- trace_0: steps 0-14 (states 0-15)
- trace_1: steps 15-29 (states 15-30)
- etc.

This ensures all algorithms see the same noisy observations.

## Evaluation Metrics

The benchmark collects:
- **Learning time**: Time to learn action model
- **Model accuracy**: Comparison with ground truth
- **Planning performance**: Success rate on test problems
- **Plan quality**: Plan length and optimality

## Results

Results are saved in:
- `results/blocks/benchmark_results.json` - Overall results
- `results/blocks/comparison.csv` - Metrics comparison table
- `results/blocks/plots/` - Visualization plots

## Implementation Status

- âœ… Stage 1: Domain equalization
- âœ… Stage 2: Training data generation
- âœ… Stage 3: LLM noise addition
- ğŸ”„ Stage 4: Experiment execution (in progress)
- â³ Stage 5: Results analysis
- â³ Stage 6: Visualization

## Notes

- The noise generation step is the slowest (10-20+ min for 101 images)
- All traces use the same LLM noise for fairness
- Training data is excluded from test set
- Evaluation uses standard PDDLGym problem sets
